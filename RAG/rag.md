# RAG explained

RAG - Retrieval Augmented Generation 

1. [Beginner's Guide to RAG](https://youtu.be/PrYuqtT43BE?si=AKGhMLHz_GbNPlnt)

    * notes from the above link:
        * mathematical equation explaining RAG
        * A `prompt` that implemented the above equation
        * `index` data in order to search and provide context for the retrieval part of rag
        * `load` - indexing involves the process pipeline of loading data
        * `split` splitting data into chunk
        * `embed` embedding each chunk into vectors 
        * `store` - storing vectors into a database
        * `retrieval` - we can retrieve information form the database


### Mathematical Equation - RAG

* RAG = Query + Prompt + Context + LLM  
    * equation helps in retrieval at best, structured way :+1:


### Index

Before Indexing Process: 

![image](https://hackmd.io/_uploads/rkbl1XcPye.png)


* `load` - we need to load documents into the pipeline from database
* `split` - splitting the document into chunks in a way that there will be tokens in each line. A paragrph broken into a chunk of x characters
    * assume we are splitting the paragraph into three characters
* `embed`- we need to perform var embedding
    * each word aka token will corespond to some vector element in the matrix as shown in the above fig.
    * Merge one entire chunk of the data into one with `syntax embedding` which means we are making three single vectors as one vector
* `store` - the last step in indexing is to store them
    * as shown in the fig, minimum of three column is required to store in vector database (do we really need 3 ?)
        * we write `chunk` in `chunk` column
        * we write `embedded vector` in `embed` column
        * we write `metadata` in `metadata` column which is basically where the data came from
        * we will store one chunk in each document

After Indexing process: 
![image](https://hackmd.io/_uploads/HJux4mqv1e.png)


### load

* for loading, there are bunch of website available, one such website is `llamahub data loader` [data loaders](https://llamahub.ai/?tab=readers)
    * There will be loaders for `csv` `word doc`

### Split

* splitting by tokens
    * ```python
        TokenTextSplitter (
            chunk_size=4 #desired chunk size
            chunk_overlap=0
        )
    * `tokensplitter` is a library, there are many libraries out there for splitting the data into chunks
    * `chunk overlap` here means overlapping with the previous chunk
        * eg: he is a fan of TFI cinema 
            * he is a fan - chunk #1
            * fan of TFI cinema - chunk #2


### Embed

* Hugging face is the best place to find embedding tools
* for checking [Massive Text Embedding Benchmark](https://huggingface.co/spaces/mteb/leaderboard) , we can choose the embedding model based on our preference, for eg: based on the model size, memory usage, embedding dimensions, context window(max tokens), classification and clustering performance

### Store

* for storing we can choose a [Vector Database](https://superlinked.com/vector-db-comparison)
* we can choose our preferred database based on the following parameters
    * Disk index
    * Ephemeral index
    * Sharding
    * Document size
    * vector Dimensions
    * Int8 Quantization
    * Binary Quantization

### Retrieval

* for retrieval, we can use database like SQL
* We will query the vector database with commands below
```sql
SELECT comment, emb<*>[0,2,2,1] AS score
FROM posts
ORDER BY score ASC;here
```

* we can even use some high level APIs which is basically query design

### Advanced RAG

![image](https://hackmd.io/_uploads/BkX-A4qw1x.png)

* when the user issues a query, without rag, we just sedn this query to `lastage model` and get the answer
* With `RAG` we augment the query with context, which means we will send the same query to database and the database is going to look up relevant documents, with the prompt that's been given, we send this query to LLM, LLM produces the output as an answer to the user which reduces the risk of hallucination
* The process: 
![image](https://hackmd.io/_uploads/H1SxlHqwkl.png)

### Multi Query Retrieval

* A user will send a query, now instead of RAG, the query is sent to LLM to ask a similar question, the query generated by the LLM is sent to the database, checks the documents, sends the output to the LLM, the LLM generates the output as the answer to the user

![image](https://hackmd.io/_uploads/BkQaMB5DJg.png)

* This is how the user query in Multi query Retrieval
    ![image](https://hackmd.io/_uploads/rkzQQBcPyx.png)

### Contextual Compression

* This is an another way to improve rag


### Hypothetical Document Embedding 
       
       ------------WIP-----------
       

#### Credits

This markdown file contains my personal notes and code written while learning from the course `Beginner's Guide to RAG` by [Prof. Tom Yeh] on [youtube](https://youtu.be/PrYuqtT43BE?si=ZPfsZxHGOYGFtdHs)