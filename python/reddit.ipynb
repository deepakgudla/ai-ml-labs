{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve credentials from environment variables\n",
    "client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "user_agent = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Subreddit name\n",
    "subreddit_name = \"cosmosnetwork\"  # Replace with the desired subreddit name\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "# Fetch the latest 10 posts\n",
    "posts = []\n",
    "for submission in subreddit.new(limit=10):  # Use .new() for the newest posts\n",
    "    posts.append(submission.title + \": \" + submission.selftext)\n",
    "\n",
    "# Print the fetched posts\n",
    "print(\"Fetched posts from subreddit:\")\n",
    "for post in posts:\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Authenticate Hugging Face (required for gated models)\n",
    "# Public models do not need this step if you skip the `use_auth_token` parameter.\n",
    "\n",
    "# Load a summarization model from Hugging Face\n",
    "# Replace with \"meta-llama/Llama-2-7b-hf\" if you have access or use a public model like t5-small.\n",
    "model_name = \"t5-small\"  # Change this to a different model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Using the posts fetched from Cell-1 (from the `posts` list)\n",
    "input_text = \"Summarize the following posts into a single tweet:\\n\" + \"\\n\".join(posts)\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=200,           # Maximum length of the summary\n",
    "    num_beams=5,             # Beam search for better quality\n",
    "    early_stopping=True      # Stop early when the result is stable\n",
    ")\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve credentials from environment variables\n",
    "client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "user_agent = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "redirect_uri=os.getenv(\"REDDIT_REDIRECT_URI\")\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    redirect_uri=redirect_uri    \n",
    ")\n",
    "\n",
    "# Step 1: OAuth2 Login\n",
    "print(\"Please visit this URL to authorize the app: \", reddit.auth.url(['identity', 'submit'], 'unique_state', 'permanent'))\n",
    "\n",
    "# After visiting the URL, the user will receive a code. Enter it here.\n",
    "code = input(\"Enter the code you received: \")\n",
    "\n",
    "# Step 2: Use the code to get the access token\n",
    "reddit.auth.authorize(code)\n",
    "\n",
    "\n",
    "# Now we are logged in and can post to the subreddit.\n",
    "\n",
    "# Subreddit name\n",
    "subreddit_name = \"cosmosnetwork\"  # Replace with the desired subreddit name\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "# Example posts fetched from Reddit (Replace this with the actual posts)\n",
    "posts = [\n",
    "    \"Post 1: Reddit's blockchain feature is launching soon.\",\n",
    "    \"Post 2: New updates from Avail's latest partnerships.\",\n",
    "    \"Post 3: Insights into modular blockchain development.\"\n",
    "]  # Replace this with your fetched posts\n",
    "\n",
    "# Summarizing the posts\n",
    "model_name = \"t5-small\"  # Change this to a different model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Prepare text for summarization\n",
    "input_text = \"Summarize the following posts into a single tweet:\\n\" + \"\\n\".join(posts)\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=50,           # Maximum length of the summary\n",
    "    num_beams=5,             # Beam search for better quality\n",
    "    early_stopping=True      # Stop early when the result is stable\n",
    ")\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)\n",
    "\n",
    "# Step 3: Post the summary to Reddit\n",
    "title = \"Summary of the latest posts in the Avail subreddit\"\n",
    "subreddit.submit(title, selftext=summary)  # Submit the summary as a post\n",
    "\n",
    "print(f\"Summary successfully posted to {subreddit_name}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
